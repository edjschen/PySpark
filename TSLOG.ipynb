{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "from ua_parser import user_agent_parser\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import expr, pandas_udf\n",
        "from pyspark.sql.types import StringType\n",
        "import pandas as pd\n",
        "import time"
      ],
      "metadata": {
        "id": "qgDh0ML2BERD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E1o5Q7rKDL3"
      },
      "source": [
        "## *Tools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Estimate_df_size(df):\n",
        "    df.cache().foreach(lambda x: x)\n",
        "    catalyst_plan = df._jdf.queryExecution().logical()\n",
        "    size_in_byte = spark._jsparkSession.sessionState().executePlan(catalyst_plan).optimizedPlan().stats().sizeInBytes()\n",
        "    print(size_in_byte / pow(1024, 3),\"GB\")"
      ],
      "metadata": {
        "id": "vmcMfiAEYqsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "nUPDKF_QKDL5"
      },
      "source": [
        "# 1. Organized TSLOG from PV log"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## list of url (exact) or url pattern (regex)\n",
        "find_list=[\"https://www.example.com/find/\", \"https?://find.example.com/\"]\n",
        "item_list=[\"https://www.example.com/item/show\"]\n",
        "home_list=[\"https://www.example.com/\", \"http://www.example.com/\"]\n",
        "mail_list=[\"https://deref-mail.com\", \"https://mail\",\"http://mail\", \"http://webmail\"]\n",
        "social_list=[\"facebook.com\", \"youtube.com\", \"instagram.com\"]\n",
        "\n",
        "## Setting\n",
        "url_regex='https?:\\/\\/(?:www\\.)?'"
      ],
      "metadata": {
        "id": "lICVm68ZKtUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "hzzaO9H2KDL8",
        "outputId": "d52af5f6-5776-47a7-8e8d-12b812e1cd0f",
        "colab": {
          "referenced_widgets": [
            ""
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read Date 20230807 Success\n",
            "Done 604.8529586791992"
          ]
        }
      ],
      "source": [
        "# Setting\n",
        "def classify_url_type(col_name, include_extended=False):\n",
        "    \"\"\"\n",
        "    Classify URL column into types (find/item/home/etc.)\n",
        "\n",
        "    Args:\n",
        "        col_name: Name of column to classify\n",
        "        include_extended: If True, include social/mail/referral classifications\n",
        "    \"\"\"\n",
        "    base_expr = F.when(F.col(col_name).rlike(\"|\".join(find_list)), 'find')\\\n",
        "        .when(F.col(col_name).rlike(\"|\".join(item_list)), 'item')\\\n",
        "        .when(F.col(col_name).isin(home_list), 'home')\n",
        "\n",
        "    if include_extended:\n",
        "        base_expr = base_expr\\\n",
        "            .when((F.col(col_name).rlike(\"|\".join(social_list)) & (F.col('source').rlike('utm_source'))), 'social_paid')\\\n",
        "            .when((F.col(col_name).rlike(\"|\".join(social_list)) & ~(F.col('source').rlike('utm_source'))), 'social')\\\n",
        "            .when((F.col(col_name).rlike(\"|\".join(mail_list)) | (F.col('source').rlike('utm_medium=email'))), 'mail')\\\n",
        "            .when(F.col(col_name).rlike(url_regex), 'referral')\n",
        "\n",
        "    return base_expr.otherwise('unknown')\n",
        "\n",
        "@pandas_udf(StringType())\n",
        "def parse_os(ua: pd.Series) -> pd.Series:\n",
        "    return ua.apply(lambda x: user_agent_parser.ParseOS(x).get('family'))\n",
        "@pandas_udf(StringType())\n",
        "def parse_browser(ua: pd.Series) -> pd.Series:\n",
        "    return ua.apply(lambda x: user_agent_parser.ParseUserAgent(x).get('family'))\n",
        "\n",
        "window=Window.partitionBy('TSID').orderBy('time')\n",
        "window_cumulative = window.rangeBetween(Window.unboundedPreceding, 0)\n",
        "\n",
        "date_list = [f'{date:%Y%m%d}' for date in pd.date_range(start='2023-08-07', end='2023-08-07', freq='D')]\n",
        "\n",
        "\n",
        "# Start Loop\n",
        "s0 = time.time()\n",
        "for date_str in date_list:\n",
        "\n",
        "    # (1) Read Data\n",
        "    try:\n",
        "        ts_log = spark.read.text('/path/to/log/{}/*.log.gz'.format(date_str))\n",
        "        df_ts = ts_log.rdd.map(lambda line:line.value.split('<t@s>')).toDF([\"time\",\"IP\",\"request\",\"status\",\"user_agent\",\"source\",\"RC\",\"CID\"])\n",
        "        print(f'Read Date {date_str} Success')\n",
        "    except Exception as e:\n",
        "        print(f'Read Date {date_str} Fail: {e}')\n",
        "        continue\n",
        "\n",
        "    # (2) Process\n",
        "    df_ts = df_ts.select(\n",
        "    '*',\n",
        "    F.regexp_extract(\"request\",'(&type=)(\\w+)',2).alias(\"type\"),\n",
        "    F.regexp_extract(\"request\",'(&_ts_id=)([0-9A-F.]+)',2).alias(\"TSID\"),\n",
        "    F.regexp_extract(\"request\",'(&ref=)([^&]+)',2).alias(\"referrer\"),\n",
        "    F.regexp_extract(\"request\",'(&ts_set=)(\\w+)',2).alias(\"ts_set\"))\n",
        "    df_ts = df_ts.withColumn('referrer', expr(\"java_method('java.net.URLDecoder', 'decode', referrer, 'UTF-8')\"))\n",
        "\n",
        "    # device - OS, device\n",
        "    df_ts=df_ts.withColumn('OS',parse_os(F.col('user_agent')))\\\n",
        "        .withColumn('ua_browser',parse_browser(F.col('user_agent')))\\\n",
        "        .withColumn('device', F.when(F.col('user_agent').like('%IOS%'),'app_ios')\\\n",
        "                        .when(F.col('user_agent').like('%Android%'),'app_android')\\\n",
        "                        .when(F.col('OS').isin(['iOS','Android']),'mobile')\\\n",
        "                        .otherwise('pc'))\n",
        "\n",
        "    # (3) Clean\n",
        "    df_ts = df_ts.filter('type=\"pv\"')\\\n",
        "             .withColumn('s_type', classify_url_type('source', include_extended=False))\\\n",
        "             .withColumn('r_type', classify_url_type('referrer', include_extended=True))\n",
        "\n",
        "    use_col = ['TSID', 'CID', 'time', 'IP', 'OS', 'ua_browser', 'device',\n",
        "              'status', 'source', 's_type', 'ts_set', 'referrer', 'r_type']\n",
        "    df_ts = df_ts.select(use_col)\n",
        "\n",
        "    # (4) session variable\n",
        "    df_ts = df_ts.withColumn('time',F.to_timestamp('time'))\\\n",
        "                 .withColumn('lead_time', F.lead('time').over(window))\\\n",
        "                 .withColumn('lag_time', F.lag('time').over(window))\\\n",
        "                 .withColumn('time_spent', F.col('lead_time').cast('long')-F.col('time').cast('long'))\\\n",
        "                 .withColumn('exit_page',F.when((F.col('time_spent')>=1800) | (F.col('time_spent').isNull()), F.lit(1)).otherwise(0))\\\n",
        "                 .withColumn('lag_exit_page', F.coalesce(F.lag('exit_page').over(window), F.lit(0)))\\\n",
        "                 .withColumn('session_id', F.sum('lag_exit_page').over(window_cumulative) + 1)\\\n",
        "                 .withColumn('time_spent', F.when(F.col('exit_page')==1, None).otherwise(F.col('time_spent')))\\\n",
        "                 .drop('lead_time','lag_time','lag_exit_page')\n",
        "\n",
        "    # (5) Save\n",
        "    df_ts.repartition(50).write.parquet(f'/path/to/output/pv_{date_str}',mode='overwrite')\n",
        "    print(f'Done: {time.time()-s0:.2f}s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g43llfttKDL_",
        "outputId": "f8b694c4-7f85-4040-c23d-040b84a07b9d",
        "colab": {
          "referenced_widgets": [
            ""
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5624896558001637 GB"
          ]
        }
      ],
      "source": [
        "df_final.columns\n",
        "Estimate_df_size(df_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyWoFMXEKDMA"
      },
      "source": [
        "## Try 1 day"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pv_example = spark.read.parquet(\"/path/to/log/pv_202304*\")\n",
        "\n",
        "pv_example.select(\"r_type\").distinct().show()\n",
        "pv_example.groupBy(\"r_type\").count().show()\n",
        "\n",
        "pv_example = pv_example.filter(F.col(\"referrer\").isNotNull() & (F.col(\"referrer\") != \"\"))\n",
        "row_count = pv_example.count()\n",
        "df = pv_example.groupBy(\"referrer\").agg(F.count(\"referrer\").alias(\"count\"),\n",
        "                                       F.first(\"r_type\").alias(\"type\"))\\\n",
        "              .withColumn(\"percent\", F.round(F.col(\"count\")/row_count * 100, 2))\\\n",
        "              .orderBy(F.col(\"count\").desc())\n",
        "\n",
        "# Increase column width for display\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.maxNumRows\", 1000)\n",
        "\n",
        "#df.show(30, truncate=False)"
      ],
      "metadata": {
        "id": "wKJJKvQrnneH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SYNTAX: F.regexp_extract(column, pattern, group_index)\n",
        "# - pattern: regex with capture groups ()\n",
        "# - group_index: 0 = entire match, 1 = first (), 2 = second (), etc.\n",
        "\n",
        "# EXAMPLE 1: Extract value after parameter name\n",
        "# r\"_content=([^&]*)\", 1\n",
        "# _content=     → match literal text\n",
        "# ([^&]*)       → group 1: capture everything until \"&\" (zero or more chars)\n",
        "# index 1       → return group 1\n",
        "# \"page?_content=electronics&other\" → \"electronics\"\n",
        "\n",
        "# EXAMPLE 2: Extract second capture group\n",
        "# r'(q=)([^&]+)', 2\n",
        "# (q=)          → group 1: match \"q=\"\n",
        "# ([^&]+)       → group 2: capture everything until \"&\" (one or more chars)\n",
        "# index 2       → return group 2\n",
        "# \"search?q=laptop&sort=price\" → \"laptop\"\n",
        "\n",
        "# EXAMPLE 3: Extract entire pattern match\n",
        "# r'(\\d+)_(\\d+)', 0\n",
        "# (\\d+)         → group 1: one or more digits\n",
        "# _             → literal underscore\n",
        "# (\\d+)         → group 2: one or more digits\n",
        "# index 0       → return entire match (not individual groups)\n",
        "# \"item_123_456_end\" → \"123_456\"\n",
        "\n",
        "# EXAMPLE 4: Extract digits after literal \"?\"\n",
        "# r'show\\?(\\d+)', 1\n",
        "# show          → match literal \"show\"\n",
        "# \\?            → match literal \"?\" (escaped)\n",
        "# (\\d+)         → group 1: one or more digits\n",
        "# index 1       → return group 1\n",
        "# \"show?12345&other\" → \"12345\"\n",
        "\n",
        "# COMMON PATTERNS:\n",
        "# \\d     → digit          [^&]   → not \"&\"\n",
        "# +      → one or more    *      → zero or more\n",
        "# \\?     → literal \"?\"    ()     → capture group\n",
        "\n",
        "\n",
        "# Extract & decode query: \"http://site.com?q=hello%20world\" → \"q=hello world\"\n",
        "# urlparse_udf = udf(lambda x: parse.unquote(parse.urlparse(x).query), StringType())\n",
        "\n",
        "# Decode URL encoding: \"hello%20world\" → \"hello world\"\n",
        "# unquote_udf = udf(lambda x: parse.unquote(x), StringType())\n",
        "\n",
        "# Create empty JSON object: {}\n",
        "# .withColumn('COL', F.to_json(F.struct()))\n",
        "\n",
        "# Merge columns to JSON: cols(a=\"123\", b=456) → {\"a\":\"123\",\"a\":3}\n",
        "# .withColumn('COL', F.to_json(F.struct(\"a\", \"b\")))\n",
        "\n",
        "# Broadcast join small table to avoid shuffle (use when right table <10MB)\n",
        "# Data = df.join(F.broadcast(Data), on=['COL'], how='inner')"
      ],
      "metadata": {
        "id": "awjrby-eN_BT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pysparkkernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "pyspark",
      "pygments_lexer": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
