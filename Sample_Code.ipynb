{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%configure -f\n",
        "{\n",
        "    \"kind\": \"pyspark\",\n",
        "    \"name\": \"edwardchen\",\n",
        "\n",
        "    // Driver configuration\n",
        "    \"driverMemory\": \"2G\",\n",
        "    \"driverCores\": 1,\n",
        "\n",
        "    // Executor configuration\n",
        "    \"executorCores\": 4,\n",
        "    \"executorMemory\": \"8G\",\n",
        "    \"numExecutors\": 4,\n",
        "\n",
        "    // Additional runtime parameters\n",
        "    \"heartbeatTimeoutInSecond\": 120,\n",
        "\n",
        "    // Load dependencies and environment archive\n",
        "    \"pyFiles\": [\"hdfs://path/data/*\"],\n",
        "\n",
        "    // YARN queue\n",
        "    \"queue\": \"default\",\n",
        "    \"archives\": [\"hdfs://path/zip/base.zip#python_env\"],\n",
        "\n",
        "    // Spark configuration\n",
        "    \"conf\": {\n",
        "        \"spark.default.parallelism\": \"16\",\n",
        "        \"spark.sql.shuffle.partitions\": \"200\",\n",
        "        \"spark.sql.auto.repartition\": \"true\",\n",
        "        \"spark.yarn.appMasterEnv.PYSPARK_PYTHON\": \"python_env/bin/python\"\n",
        "     }\n",
        "}"
      ],
      "metadata": {
        "id": "-BZ_-KUGMSz5"
      },
      "id": "-BZ_-KUGMSz5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "# spark\n",
        "# spark.version"
      ],
      "metadata": {
        "id": "p1XiKRJOMWxx"
      },
      "id": "p1XiKRJOMWxx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ae9d6266-de1e-41c0-8868-752b5bde3741",
      "metadata": {
        "id": "ae9d6266-de1e-41c0-8868-752b5bde3741"
      },
      "source": [
        "# Time distribution of first purchase after register\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users = spark.read.parquet('/path/to/data')\n",
        "users = users.select('REG_DATE','ID', 'USER_STATUS','USER')\\\n",
        "             .filter(\n",
        "                    (F.col('REG_DATE').between('20230401', '20230531')) &\n",
        "                    (F.col('USER_STATUS').isin([0,6])) &\n",
        "                    (~F.col('USER').rlike('@random')))\\\n",
        "             .drop('USER_STATUS','USER')\n",
        "\n",
        "orders = spark.read.parquet('/path/to/data')\n",
        "orders = orders.select('P_ID','ORDER_DATE', 'G_CLASS', 'G_NAME', 'G_NO', 'TOTAL_AMOUNT')\\\n",
        "              .withColumnRenamed('P_ID', 'ID')\n",
        "\n",
        "# users.show(5, truncate=False)\n",
        "# orders.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "yMSgFUKZiTLy"
      },
      "id": "yMSgFUKZiTLy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df = users.join(orders, on='ID', how='left')\n",
        "joined_df = joined_df.filter(F.col(\"REG_DATE\").isNotNull())\\\n",
        "                     .dropDuplicates()\\\n",
        "                     .withColumn(\"DATE\", F.to_date(\"ORDER_DATE\", \"yyyyMMdd\"))\\\n",
        "                     .withColumn(\"REG_DATE\", F.to_date(\"REG_DATE\", \"yyyyMMdd\"))\\\n",
        "                     .withColumn('TIME_DIF', F.datediff(\"DATE\", \"REG_DATE\"))\\\n",
        "                     .cache()\n",
        "\n",
        "# joined_df.show(5, truncate=False)\n",
        "# joined_df.printSchema()"
      ],
      "metadata": {
        "id": "JBv1MSNqtV30"
      },
      "id": "JBv1MSNqtV30",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_spec = Window.partitionBy(\"ID\").orderBy(\"DATE\")\n",
        "joined_df = joined_df.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
        "count_df = joined_df.filter(F.col(\"row_num\") == 1).drop(\"row_num\")\n",
        "\n",
        "print(\"Number of rows in df:\", count_df.count())"
      ],
      "metadata": {
        "id": "Pt8tAAFCtXr9"
      },
      "id": "Pt8tAAFCtXr9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%spark -o count_df -n 30000"
      ],
      "metadata": {
        "id": "3veXF3AhRPA-"
      },
      "id": "3veXF3AhRPA-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%local\n",
        "\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Percent NA\n",
        "total_count = len(count_df)\n",
        "count_NaN = count_df[\"TIME_DIF\"].isna().sum()\n",
        "percentage_NaN = (count_NaN / total_count) * 100\n",
        "print(percentage_NaN)\n",
        "\n",
        "## By date\n",
        "for day in [0, 7, 14, 30, 60, 90, 180]:\n",
        "    percentage = (count_df[\"TIME_DIF\"] <= day).sum() / total_count * 100\n",
        "    print(percentage)\n",
        "\n",
        "value_counts = count_df[\"TIME_DIF\"].value_counts().sort_index()\n",
        "\n",
        "# Plot using Matplotlib\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(value_counts.index, value_counts.values)\n",
        "plt.xlabel('Days (TIME_DIF)')\n",
        "plt.ylabel('Number of Users')\n",
        "plt.title('Time to First Purchase Distribution')\n",
        "plt.xlim(0, 30)\n",
        "\n",
        "plt.savefig(\"graph.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Save count_df\n",
        "count_df.to_csv('output.csv', index=False)"
      ],
      "metadata": {
        "id": "Gx-IItRsRRAn"
      },
      "id": "Gx-IItRsRRAn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5c24fa41-c23d-48ab-86d0-6de6e8db6067",
      "metadata": {
        "id": "5c24fa41-c23d-48ab-86d0-6de6e8db6067"
      },
      "source": [
        "# Average Order and Amount\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trans = spark.read.parquet('/path/to/data')\n",
        "trans = trans.select('TRANSACTION_NO', 'P_ID', F.col('TOTAL_AMOUNT').cast('double'), 'INSERT_TIME', 'TRANS_CANCELFLAG')\\\n",
        "             .withColumn(\"DATE\", F.to_date(\"INSERT_TIME\", \"yyyyMMdd\"))\\\n",
        "             .filter(F.col(\"DATE\").between(\"2023-04-01\", \"2023-05-31\"))\\\n",
        "             .filter(F.col(\"TRANS_CANCELFLAG\") == F.lit(\"N\"))\\\n",
        "             .withColumnRenamed(\"P_ID\", \"ID\") \\\n",
        "             .filter(F.col(\"TOTAL_AMOUNT\") <= 200000) \\\n",
        "             .drop(\"INSERT_TIME\") \\\n",
        "             .cache()\n",
        "\n",
        "#trans.show(5, truncate=False)\n",
        "#trans.agg(sum(\"TOTAL_AMOUNT\").alias(\"total_sales\")).show()"
      ],
      "metadata": {
        "id": "8kJR4PPvRS1y"
      },
      "id": "8kJR4PPvRS1y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Daily purchase amount and order\n",
        "df = trans.join(joined_df, on=[\"ID\", \"DATE\"], how='inner')\n",
        "\n",
        "#Check duplicate\n",
        "duplicate_count = df.groupBy(\"TRANSACTION_NO\").count().filter(F.col(\"count\") > 1)\n",
        "if duplicate_count.count() > 0:\n",
        "    print(\"Duplicate values found in the 'TRANSACTION_NO' column:\")\n",
        "    duplicate_count.show()\n",
        "else:\n",
        "    print(\"No duplicate values found in the 'TRANSACTION_NO' column.\")\n",
        "\n",
        "df = df.dropDuplicates([\"TRANSACTION_NO\"])"
      ],
      "metadata": {
        "id": "Dsj_JWbdRURJ"
      },
      "id": "Dsj_JWbdRURJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum_df = df.groupBy(\"ID\", \"DATE\").agg(F.sum(\"TOTAL_AMOUNT\").alias(\"TOTAL_AMOUNT_DAY\"),\n",
        "                                      F.count(\"TRANSACTION_NO\").alias(\"NUM_TRANS\"),\n",
        "                                      F.first(\"TIME_DIF\", True).alias(\"TIME_DIF\"))\\\n",
        "                                      .withColumn(\"TIME_DIF_FIRST\", F.first(\"TIME_DIF\").over(window_spec))\\\n",
        "                                      .cache()\n",
        "\n",
        "#sum_df.show()"
      ],
      "metadata": {
        "id": "6DhuYR10lbLJ"
      },
      "id": "6DhuYR10lbLJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "abbc5aea-8118-4ccc-b026-f1e203ed81e2",
      "metadata": {
        "id": "abbc5aea-8118-4ccc-b026-f1e203ed81e2"
      },
      "source": [
        "# Repurchase and Customer Lifetime Value"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "value_df = sum_df.groupBy(\"TIME_DIF_FIRST\").agg(F.sum(\"TOTAL_AMOUNT_DAY\").alias(\"Sales_By_TIME_DIF\"),\n",
        "                                                F.countDistinct(\"ID\").alias(\"PEOPLE_COUNT\"))\\\n",
        "                                                .orderBy(F.asc(\"TIME_DIF_FIRST\"))\n",
        "\n",
        "distribution = value_df.describe(\"PEOPLE_COUNT\")\n",
        "distribution.show()\n",
        "\n",
        "# Calculate quantile\n",
        "quantiles_result = value_df.approxQuantile(\"PEOPLE_COUNT\", [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], 0.01)\n",
        "for i, q_val in enumerate(range(10, 100, 10)):\n",
        "    print(f\"Quantile {q_val}%: {quantiles_result[i]}\")\n",
        "\n",
        "# Quantile 10%: 43.0\n",
        "# Quantile 20%: 234.0\n",
        "# Quantile 30%: 801.0\n",
        "# Quantile 40%: 2301.0\n",
        "# Quantile 50%: 4940.0\n",
        "# Quantile 60%: 8083.0\n",
        "# Quantile 70%: 12807.0\n",
        "# Quantile 80%: 18519.0\n",
        "# Quantile 90%: 26754.0"
      ],
      "metadata": {
        "id": "5kdlJw1qylzL"
      },
      "id": "5kdlJw1qylzL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ba297b5a",
      "metadata": {
        "id": "ba297b5a"
      },
      "source": [
        " # Total Sales of First-Time Buyer Products -(start here)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Include First-Time Purchasers Within 10 Days of Registration\n",
        "# .orderBy(F.col(\"TOTAL_AMOUNT\").desc()) =  .orderBy(F.desc(\"TOTAL_AMOUNT\")\n",
        "goodno_df = df.withColumn(\"TIME_DIF_FIRST\", F.first(\"TIME_DIF\", True).over(window_spec))\\\n",
        "              .filter(F.col(\"TIME_DIF\").isNotNull())\\\n",
        "              .filter(F.col(\"TIME_DIF\") == F.col(\"TIME_DIF_FIRST\"))\\\n",
        "              .filter(F.col('TIME_DIF_FIRST') <= 10)\\\n",
        "              .withColumn(\"G_ClASS4\", F.substring(F.col(\"G_CLASS\"), 1, 4))\\\n",
        "              .groupBy(\"G_NO\").agg(F.sum(\"TOTAL_AMOUNT\").alias(\"TOTAL_AMOUNT\"),\n",
        "                                   F.first(\"G_NAME\").alias(\"Name\"),\n",
        "                                   F.first(\"G_ClASS4\").alias(\"G_ClASS4\"))\\\n",
        "              .orderBy(F.desc(\"TOTAL_AMOUNT\"))\n",
        "\n",
        "#goodno_df.limit(100).show(10)"
      ],
      "metadata": {
        "id": "Fj6RYJmmjvS7"
      },
      "id": "Fj6RYJmmjvS7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "634c5b45",
      "metadata": {
        "id": "634c5b45"
      },
      "outputs": [],
      "source": [
        "%spark -o goodno_df  -n 100\n",
        "%spark -o value_df   -n 1500"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%local\n",
        "import pandas as pd\n",
        "\n",
        "# Set display options to show all rows and columns\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.width\", None)\n",
        "\n",
        "display(goodno_df)\n",
        "display(value_df)"
      ],
      "metadata": {
        "id": "EjTXrJfvRWgw"
      },
      "id": "EjTXrJfvRWgw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Log Data"
      ],
      "metadata": {
        "id": "vtQS195dh9Dg"
      },
      "id": "vtQS195dh9Dg"
    },
    {
      "cell_type": "code",
      "source": [
        "pv_example = spark.read.parquet(\"/user/pv_202304*\")\n",
        "pv_example5 = spark.read.parquet(\"/user/pv_202305*\")\n",
        "pv_combined = pv_example.unionByName(pv_example5, allowMissingColumns=True)\n",
        "print (pv_combined.count(),pv_combined.columns)\n",
        "\n",
        "pv = pv_combined.filter((F.col(\"NAME\").like(\"%Diet%\") & F.col(\"NAME\").like(\"%Soda%\")) | (F.col(\"NAME\").like(\"%Zero%\") & F.col(\"NAME\").like(\"%Soda%\")))\\\n",
        "                .filter(F.col('TYPE').isin('Search', 'Online', 'Amazon'))\\\n",
        "                .withColumn(\"G_NO\", F.get_json_object(F.col(\"col_name\"), \"$.key\"))\\\n",
        "                .fillna(0, subset=[\"AMOUNT\"])\\\n",
        "                .dropDuplicates([\"ID\", \"DATE\"])\n",
        "\n",
        "names = pv.select(\"NAME\").distinct().limit(100).collect()\n",
        "for row in names:\n",
        "    print(row[\"NAME\"])"
      ],
      "metadata": {
        "id": "8OMzO0Zyh7u3"
      },
      "id": "8OMzO0Zyh7u3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#count\n",
        "items = [\"20240614\", \"20240615\", \"20240616\"]\n",
        "\n",
        "filter_condition = F.col(\"G_NO\").contains(items[0])\n",
        "for item in items[1:]:\n",
        "     filter_condition |= F.col(\"G_NO\").contains(item)\n",
        "\n",
        "pv = pv.filter(filter_condition)\n",
        "\n",
        "for item in items:\n",
        "    count_result = pv.filter(F.col(\"G_NO\").contains(item)) \\\n",
        "                     .agg(F.count(\"*\").alias(\"count\")) \\\n",
        "                     .collect()[0][\"count\"]\n",
        "    print(f\"{item}: {count_result}\")"
      ],
      "metadata": {
        "id": "E0RZQBRyiMmy"
      },
      "id": "E0RZQBRyiMmy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = (\n",
        "    pv.join(orders, (orders[\"DATE\"] == pv[\"DATE\"]) & (orders[\"ID\"] == pv[\"CTRL_ROWID\"]), how='left')\\\n",
        "      .groupBy(\"ID\")\\\n",
        "      .agg(\n",
        "          F.first(\"SELLER_NAME\").alias(\"SELLER_NAME\"),\n",
        "          F.sum(orders[\"TOTAL_AMOUNT\"]).alias(\"GMV\"),\n",
        "          F.avg(F.when(F.col(\"MONTH\").between(4, 5), orders[\"TOTAL_AMOUNT\"])).alias(\"AVG_GMV\"),\n",
        "          F.sum(F.when(F.col(\"G_NO\") == \"(null)\", 1).otherwise(0)).alias(\"on_stock\"),\n",
        "          F.min(pv[\"DATE\"]).alias(\"EARLIEST_DATE\"),\n",
        "          F.count('*').alias('COUNT'),\n",
        "          F.avg(orders[\"TOTAL_AMOUNT\"]).alias(\"AVERAGE_PRICE\").alias(\"AVERAGE_PRICE\")))\\\n",
        "       .withColumn(\"GMV_bin\",\n",
        "             F.when(F.col(\"GMV\") < 1000, \"Low\")\n",
        "            .when(F.col(\"GMV\").between(1000, 5000), \"Medium\")\n",
        "            .otherwise(\"High\"))\\\n",
        "      .orderBy(\"ID\")\n",
        "\n",
        "total_rows = result.count()\n",
        "result = result.withColumn(\"percentage\", (F.col(\"COUNT\") / total_rows) * 100)"
      ],
      "metadata": {
        "id": "QUjs4O6BikbS"
      },
      "id": "QUjs4O6BikbS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gmv_month = result.withColumn(\"EARLIEST_MONTH\", F.month(F.col(\"EARLIEST_DATE\")))\\\n",
        "                  .groupBy(\"EARLIEST_MONTH\")\\\n",
        "                  .pivot(\"GMV_bin\", [\"Low\", \"Medium\", \"High\"])\\\n",
        "                  .agg(F.count(\"*\"))\\\n",
        "                  .fillna(0)\\\n",
        "                  .orderBy(\"EARLIEST_MONTH\")\n",
        "\n",
        "#gmv_month.show(30, truncate=False)\n",
        "# +--------------+-----+------+-----+\n",
        "# |EARLIEST_MONTH|Low  |Medium|High |\n",
        "# +--------------+-----+------+-----+\n",
        "# |1             |10   |5     |2    |\n",
        "# |2             |8    |6     |1    |\n",
        "# |3             |15   |8     |5    |\n",
        "# |4             |12   |10    |4    |\n",
        "# +--------------+-----+------+-----+"
      ],
      "metadata": {
        "id": "n_ryvuDFGol-"
      },
      "id": "n_ryvuDFGol-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document"
      ],
      "metadata": {
        "id": "6gAGMYGXkLHA"
      },
      "id": "6gAGMYGXkLHA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a window partitioned by id, ordered by date\n",
        "# window_spec = Window.partitionBy(\"ID\").orderBy(\"DATE\")\n",
        "\n",
        "# Example DataFrame\n",
        "# df: ID | DATE | CATEGORY | VALUE\n",
        "\n",
        "# df_new = df.withColumn(\"SUM\",          F.sum(\"VALUE\").over(window_spec))\\                                       # Cumulative sum of flag per user\n",
        "#            .withColumn(\"MAX_VALUE\",    F.max(\"VALUE\").over(window_spec))\\                                       # Max value per user\n",
        "#            .withColumn(\"CATEGORY\",     F.lead(\"CATEGORY\").over(window_spec))\\                                   # Next category in order for each user\n",
        "#            .withColumn(\"MIN_VALUE\",    F.min(\"VALUE\").over(window_spec))\\                                       # Minimum value per user\n",
        "#            .withColumn(\"SECOND_ROW\",   F.when(F.row_number().over(window_spec) == 1, 1).otherwise(0))\\          # Mark first row\n",
        "#            .withColumn(\"FIRST_DATE_C\", F.first(F.when(\"VALUE\" <= 1000, F.col(\"DATE\")), True).over(window_spec)) # First non-null date where VALUE <= 1000\n",
        "#            .withColumn(\"LAST_VALUE\",   F.last(\"VALUE\").over(window_spec))\\                                      # Return last non-null value in window partition\n",
        "\n",
        "# F.lpad(string, 2, \"0\")                                      # Pad a string on the left to length 2 with character \"0\"\n",
        "# F.month(date)                                               # Extract the month from a DateType or Timestamp\n",
        "# F.dayofmonth(date)                                          # Extract the day of the month from a DateType or Timestamp\n",
        "# F.year(date)                                                # Extract year from a DateType or Timestamp\n",
        "# F.concat(string_1, string_2, string_3)                      # Combine multiple StringType columns into one\n",
        "# F.unix_timestamp(string, \"format\")                          # Convert a string with specified format to Unix timestamp\n",
        "# F.from_unixtime(timestamp, \"format\")                        # Convert a Unix timestamp to formatted date string\n",
        "# F.date_format(date, \"format\")                               # Format a DateType or TimestampType column into a string with specified pattern\n",
        "# (F.col(\"boolean_1\") | F.col(\"boolean_2\"))                   # Logical OR between two BooleanType columns\n",
        "# F.to_date(\"20230401\", \"yyyyMMdd\")                           # Convert string to DateType; format \"yyyyMMdd\" for strings like \"20230401\", \"yyyy-MM-dd\" for \"2023-04-01\"\n",
        "# F.to_timestamp(\"2023-04-01 14:30:00\",\"yyyy-MM-dd HH:mm:ss\") # Convert string to TimestampType with specified format\n",
        "# F.substring(F.col(\"text\"), -10, 10)                         # Extract last 10 characters (negative index counts from end)\n",
        "# F.col(\"VALUE\").isNull()                                     # Check if column value is NULL\n",
        "# F.isnan(F.col(\"VALUE\"))                                     # Check if numeric column value is NaN (Not a Number)"
      ],
      "metadata": {
        "id": "MAH7wMafkMkO"
      },
      "id": "MAH7wMafkMkO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pysparkkernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "pyspark",
      "pygments_lexer": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
